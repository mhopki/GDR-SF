<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Real-Time Glass Detection and Reprojection using Sensor Fusion on Aerial Robots</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 2em auto;
      padding: 1em;
      line-height: 1.6;
    }
    h1, h2 {
      text-align: center;
    }
    img {
      max-width: 100%;
      height: auto;
      /* --- Add these two lines to center the images --- */
      display: block; /* Allows margin: auto to work */
      margin: 0 auto; /* Centers a block-level element horizontally */
    }
    .button {
      display: inline-block;
      padding: 0.5em 1em;
      margin: 0.5em;
      border-radius: 5px;
      background-color: #333;
      color: white;
      text-decoration: none;
    }
    .button:hover {
      background-color: #555;
    }
    pre {
      background: #f4f4f4;
      padding: 1em;
      overflow-x: auto;
    }
    .image-row {
      display: flex;
      justify-content: center;
      gap: 1em;
      margin: 1em 0;
      flex-wrap: wrap;
    }
    .image-row img {
      width: 30%;
      min-width: 200px;
    }
  </style>
</head>
<body>

  <div style="text-align: center;">
    <h1>Real-Time Glass Detection and Reprojection using Sensor Fusion on Aerial Robots</h1>

    <h2>Authors</h2>
    <p>
      Malakhi Hopkins<sup>1</sup>, Varun Murali<sup>2</sup>, Vijay Kumar<sup>1</sup>, Camillo J Taylor<sup>1</sup><br>
      <sup>1</sup>GRASP Laboratory, University of Pennsylvania, Pennsylvania, USA&nbsp;&nbsp;
      <sup>2</sup>Texas A&M University, Texas, USA&nbsp;&nbsp;
    </p>

    <a href="X" class="button" target="_blank">View on arXiv</a>
    <a href="X" class="button" target="_blank">Watch Video</a>
  </div>

  <h2>Abstract</h2>
  <p>
    Autonomous aerial robots are increasingly being deployed in real-world scenarios, where transparent obstacles present significant challenges to reliable navigation and mapping. These materials pose a unique problem for traditional perception systems because they lack discernible features and can cause conventional depth sensors to fail, leading to inaccurate maps and potential collisions. To ensure safe navigation, robots must be able to accurately detect and map these transparent obstacles. Existing methods often rely on large, expensive sensors or algorithms that impose high computational burdens, making them unsuitable for low Size, Weight, and Power (SWaP) robots. In this work, we propose a novel and computationally efficient framework for detecting and mapping transparent obstacles onboard a sub-300g quadrotor. Our method fuses data from a Time-of-Flight (ToF) camera and an ultrasonic sensor with a custom, lightweight 2D convolution model. This specialized approach accurately detects specular reflections and propagates their depth into corresponding empty regions of the depth map, effectively rendering transparent obstacles visible. The entire pipeline operates in real-time, utilizing only a small fraction of a CPU core on an embedded processor. We validate our system through a series of experiments in both controlled and real-world environments, demonstrating the utility of our method through experiments where the robot maps indoor environments containing glass. Our work is, to our knowledge, the first of its kind to demonstrate a real-time, onboard transparent obstacle mapping system on a low-SWaP quadrotor using only the CPU.
  </p>

  <h2>Glass Detector and Reprojector - Sensor Fusion (GDR-SF)</h2>
  <img src="AutonomousFlight.gif" alt="Autonomous flight" style="width: 80%;">

  <h2>System Pipeline</h2>
  <img src="SystemPipeline.jpg" alt="System Pipeline">
  <p>
    The proposed four-component system first filters the depth image based on the sonar measurement. This filtered image is simultaneously passed to the Speckle Detection algorithm (which provides 3D coordinates and normal distance) and the Depth Segmentation algorithm (which bounds potential glass regions). Finally, the outputs are fused in the Transparent Plane Detection algorithm, which fills the empty space with the speckle's depth and tilts the plane by the estimated normal angle.
  </p>

  <h2>Results: Comparison to Existing Methods</h2>
  <img src="ResultsComparison.jpg" alt="Results Comparison">
  <p>
    We evaluate our algorithm against the existing learning algorithms GDNet and GlassSemNet. In our single glass pane detection and segmentation, head-on experiments we accomplish superior metrics in all 3 categories, and in our angled (5 degrees) experiments achieve superior precision and mIOU. Our algorithm also acheives vastly superior processing rate and CPU usage.
  </p>

  <h2>BibTeX</h2>
  <pre><code>X, 
}</code></pre>

</body>
</html>
